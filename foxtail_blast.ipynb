{
 "metadata": {
  "name": "",
  "signature": "sha256:8c37507b92365b070bfe62e0cef0f6685bef1cc45dbe87d66b83aaa23ffa4666"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Foxtail Blasting\n",
      "\n",
      "This will create a file of contigs that have SNPs in them, so only the relevant contigs will be blasted against the 1.01 version of the loblolly genome.  The actual blasting will occur on an iPlant atmosphere instance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from Bio import SeqIO\n",
      "import os\n",
      "from Bio.Blast import NCBIXML\n",
      "import matplotlib.pyplot as plt\n",
      "from BCBio import GFF\n",
      "from pprint import pprint\n",
      "import dill\n",
      "from IPython.display import FileLink\n",
      "import xml.etree.ElementTree as ET\n",
      "import stopwatch"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "snp_file = \"/home/cfriedline/final_maps_cleaned.txt.cf.txt\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "snp_data = pd.read_csv(snp_file, sep=\"\\t\", index_col=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "contigs = set(snp_data.index)\n",
      "len(contigs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembly = \"/home/cfriedline/data7/assemblies/foxtail2/Green_26_ATCGCGCAA.fastq_31_data_31/contigs.fa\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 0\n",
      "out = open(os.path.join(os.path.dirname(assembly), \"contigs_with_snps.fa\"), \"w\")\n",
      "reads = []\n",
      "for read in SeqIO.parse(assembly, \"fasta\"):\n",
      "    if read.id in contigs:\n",
      "        reads.append(read)\n",
      "        count += 1\n",
      "SeqIO.write(reads, out, \"fasta\")\n",
      "out.close()\n",
      "print out.name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!grep -c \">\" {out.name}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blast_file = os.path.join(os.path.dirname(assembly), \"contigs_with_snps.fa_blast.xml\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "timer = stopwatch.Timer()\n",
      "query_percs = []\n",
      "id_percs = []\n",
      "good_hits  = 0\n",
      "query_min = 50\n",
      "id_min = 75\n",
      "total_recs = 0\n",
      "query_lens = []\n",
      "for i, record in enumerate(NCBIXML.parse(open(blast_file))):\n",
      "    if len(record.alignments) > 0:\n",
      "        snp_data.loc[record.query,\"hits\"] = \"\"\n",
      "        hit_defs = set()\n",
      "        for aln in record.alignments:\n",
      "            for hsp in aln.hsps:\n",
      "                hit_defs.add(\"%s:%d:%d\" % (aln.hit_def, hsp.sbjct_start, hsp.sbjct_end))\n",
      "                query_perc = ((hsp.query_end - hsp.query_start)+1)*100.0/record.query_length\n",
      "                query_percs.append(query_perc)\n",
      "                query_lens.append(record.query_length)\n",
      "                id_perc = hsp.identities*100.0/hsp.align_length\n",
      "                id_percs.append(id_perc)\n",
      "                if query_perc >= query_min and id_perc >= id_min:\n",
      "                    good_hits += 1\n",
      "            snp_data.loc[record.query,\"hits\"] = \"|\".join(hit_defs)\n",
      "    total_recs += 1\n",
      "#     if i == 100:\n",
      "#         break\n",
      "timer.stop()\n",
      "print \"found %d good hits out of %d records (%.2f%%) in %s \" % (good_hits, total_recs, good_hits*100.0/total_recs, timer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = plt.gcf()\n",
      "f.set_size_inches(15, 5)\n",
      "plt.subplot(1,3,1)\n",
      "plt.hist(query_lens)\n",
      "plt.title(\"query lens (%.2f, [%d,%d])\" % (np.mean(query_lens), np.min(query_lens), np.max(query_lens)))\n",
      "plt.subplot(1,3,2)\n",
      "plt.hist(query_percs)\n",
      "plt.title(\"query percent (%.2f, [%d,%d])\" % (np.mean(query_percs), np.min(query_percs), np.max(query_percs)))\n",
      "plt.xlabel(\"n = %d\" % len(query_percs))\n",
      "plt.subplot(1,3,3)\n",
      "plt.hist(id_percs)\n",
      "plt.title(\"identity percent (%.2f, [%d,%d])\" % (np.mean(id_percs), np.min(id_percs), np.max(id_percs)))\n",
      "plt.xlabel(\"n = %d\" % len(id_percs))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "snp_data.hits"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gff_dir = \"/home/cfriedline\"\n",
      "gff_files = ['ptaeda.v1.01.scaffolds.trimmed.all.genes.highq_partial.gff',\n",
      "             'ptaeda.v1.01.scaffolds.trimmed.all.genes.highq_whole.gff',\n",
      "             'ptaeda.v1.01.scaffolds.trimmed.all.genes.lowq_whole.gff',\n",
      "             'ptaeda.v1.01.scaffolds.trimmed.all.genes.lowq_partial.gff']\n",
      "gff_files = [os.path.join(gff_dir, x) for x in gff_files]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#this cell crushes the kernel\n",
      "timer = stopwatch.Timer()\n",
      "gff_data = {}\n",
      "for gff in gff_files:\n",
      "    print gff\n",
      "    inner = {}\n",
      "    inner['data'] = list(GFF.parse(gff))\n",
      "    gff_data[os.path.basename(gff)] = inner\n",
      "timer.stop()\n",
      "print timer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for key, v in gff_data.items():\n",
      "    data = v['data']\n",
      "    index = {}\n",
      "    v['index'] = index\n",
      "    print key, len(data)\n",
      "    for elem in data:\n",
      "        index[elem.id] = elem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def split_hit(series):\n",
      "    try:\n",
      "        float(series.hits)\n",
      "        return series.hits\n",
      "    except:\n",
      "        return [x.split(\":\") for x in str(series.hits).split(\"|\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hits = snp_data.apply(split_hit, axis=1).dropna()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 0\n",
      "for hit in hits:\n",
      "    index = hits.index[count]\n",
      "    hit_anno = []\n",
      "    for triplet in hit:\n",
      "        hit_coords = sorted([int(triplet[1]), int(triplet[2])])\n",
      "        for key, data in gff_data.items():\n",
      "            if triplet[0] in data['index']:\n",
      "                rec = data['index'][triplet[0]]\n",
      "                for f in rec.features:\n",
      "                    genome_coords = sorted([f.location.nofuzzy_start, f.location.nofuzzy_end])\n",
      "                    if (hit_coords[0] >= genome_coords[0]) and (hit_coords[1] <= genome_coords[1]):\n",
      "                        hit_anno.append(str(f.qualifiers))\n",
      "    snp_data.loc[index, 'annotation'] = str(hit_anno)\n",
      "    count+=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k, v in snp_data.annotation.to_dict().items():\n",
      "    try:\n",
      "        float(v)\n",
      "    except:\n",
      "        data = eval(v)\n",
      "        if len(data) > 0:\n",
      "            for elem in data:\n",
      "                d = eval(elem)\n",
      "                if 'Ontology_term' in d:\n",
      "                    snp_data.loc[k, 'ontology_term'] = str(d['Ontology_term'])\n",
      "                if 'Dbxref' in d:\n",
      "                    snp_data.loc[k, 'dbxref'] = str(d['Dbxref'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out_file = snp_file+\".cf.txt\"\n",
      "snp_data.to_csv(snp_file+\".cf.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Start here if you're skipping gff data\n",
      "\n",
      "Presumably b/c you've already run and imported it from above.  Hack to get around the kernel getting bogged down when the gff data is loaded"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "go_terms = snp_data.ontology_term.dropna().apply(lambda x: eval(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "go_set = set()\n",
      "go_list = []\n",
      "for val in go_terms:\n",
      "    for elem in val:\n",
      "        go_set.add(elem)\n",
      "        go_list.append(elem)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import MySQLdb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "See http://wiki.geneontology.org/index.php/Example_Queries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conn = MySQLdb.connect(host='mysql.ebi.ac.uk', \n",
      "                       user='go_select', \n",
      "                       passwd='amigo', \n",
      "                       db='go_latest', \n",
      "                       port=4085)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "go_results = {}\n",
      "for i, term_id in enumerate(sorted(go_list)):\n",
      "    print \"%d/%d\" % (i, len(go_list))\n",
      "    cur = conn.cursor()\n",
      "    cur.execute(\"SELECT * FROM term WHERE acc='%s'\" % term_id)\n",
      "    for row in cur.fetchall():\n",
      "        print \"row %s\" % str(row)\n",
      "        ont = row[2]\n",
      "        term = row[1]\n",
      "        if not ont in go_results:\n",
      "            go_results[ont] = {}\n",
      "        else:\n",
      "            if not term in go_results[ont]:\n",
      "                go_results[ont][term] = 1\n",
      "            else:\n",
      "                go_results[ont][term] += 1\n",
      "    cur.close()\n",
      "conn.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "go_out = \"go_res.txt\"\n",
      "with open(go_out, \"w\") as o:\n",
      "    o.write(\"ont\\tterm\\tcount\\n\")\n",
      "    for ont, v in go_results.items():\n",
      "        for term, count in v.items():\n",
      "            o.write(\"%s\\t%s\\t%d\\n\" % (ont, term, count))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileLink(go_out)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interpro = 0\n",
      "for x in snp_data.dbxref.dropna().apply(lambda x: [x for x in set(eval(x)) if 'InterPro' in x]):\n",
      "    interpro += len(x)\n",
      "print interpro"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Work with InterPro"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lxml import etree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interpro = etree.parse(\"/home/cfriedline/interpro/interpro.xml\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root = interpro.getroot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anno = snp_data.annotation.dropna().apply(lambda x: eval(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_interpro_data(interpro_id):\n",
      "    node = root.find(\".//interpro[@id='%s']\" % interpro_id)\n",
      "    if node is not None:\n",
      "        name = node.find('name').text\n",
      "        return name\n",
      "#         classification = node.findall('class_list/classification')\n",
      "#         for c in classification:\n",
      "#             class_type = c.get('class_type')\n",
      "#             if class_type==\"GO\":\n",
      "#                 return name, c.get('id'), class_type, c.find('category').text, c.find('description').text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ip_key = \"Dbxref\"\n",
      "for i, x in enumerate(anno):\n",
      "    if len(x) > 0:\n",
      "        for elem in x:\n",
      "            elem = eval(elem)\n",
      "            if ip_key in elem:\n",
      "                ip_ids = [x.split(\":\")[1] for x in set(elem[ip_key]) if 'InterPro' in x]\n",
      "                for ip_id in ip_ids:\n",
      "                    print anno.index[i], find_interpro_data(ip_id)       "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    }
   ],
   "metadata": {}
  }
 ]
}