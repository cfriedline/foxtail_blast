{
 "metadata": {
  "name": "",
  "signature": "sha256:77f91e8de37ccc022eb8a048c1708502106a5a497985a2b0922e410b909e1bd5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Foxtail Blasting\n",
      "\n",
      "This will create a file of contigs that have SNPs in them, so only the relevant contigs will be blasted against the 1.01 version of the loblolly genome.  The actual blasting will occur on an iPlant atmosphere instance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from Bio import SeqIO\n",
      "import os\n",
      "from Bio.Blast import NCBIXML\n",
      "import matplotlib.pyplot as plt\n",
      "from BCBio import GFF\n",
      "from pprint import pprint\n",
      "import dill\n",
      "from IPython.display import FileLink\n",
      "import xml.etree.ElementTree as ET\n",
      "import stopwatch\n",
      "from __future__ import division\n",
      "import rpy2.ipython\n",
      "import rpy2.robjects as ro\n",
      "%matplotlib inline\n",
      "from Bio import SeqFeature\n",
      "from Bio.SeqFeature import FeatureLocation, ExactPosition"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#snp_file = \"/home/cfriedline/final_maps_cleaned.txt.cf.txt\"\n",
      "#snp_file = \"/home/cfriedline/final_maps_cleaned.txt.cf.txt\"\n",
      "snp_file = \"/home/cfriedline/final_maps_cleaned_10_28_2014.xlsx\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "snp_data = pd.read_excel(snp_file, index_col=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "snp_data[0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "contigs = set(snp_data.index)\n",
      "len(contigs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "20655"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembly = \"/home/cfriedline/data7/assemblies/foxtail2/Green_26_ATCGCGCAA.fastq_31_data_31/contigs.fa\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 0\n",
      "out = open(os.path.join(os.path.dirname(assembly), \"contigs_with_snps.fa\"), \"w\")\n",
      "reads = []\n",
      "for read in SeqIO.parse(assembly, \"fasta\"):\n",
      "    if read.id in contigs:\n",
      "        reads.append(read)\n",
      "        count += 1\n",
      "SeqIO.write(reads, out, \"fasta\")\n",
      "out.close()\n",
      "print out.name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "/home/cfriedline/data7/assemblies/foxtail2/Green_26_ATCGCGCAA.fastq_31_data_31/contigs_with_snps.fa"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!grep -c \">\" {out.name}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "20655 "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blast_file = os.path.join(os.path.dirname(assembly), \"contigs_with_snps.fa_blast.xml\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "timer = stopwatch.Timer()\n",
      "query_percs = []\n",
      "id_percs = []\n",
      "good_hits  = 0\n",
      "good_queries = 0\n",
      "query_min = 50\n",
      "id_min = 75\n",
      "total_recs = 0\n",
      "query_lens = []\n",
      "good_hit_info = {'len':[], 'len_perc':[], 'id_perc':[]}\n",
      "for i, record in enumerate(NCBIXML.parse(open(blast_file))):\n",
      "    if len(record.alignments) > 0:\n",
      "        snp_data.loc[record.query,\"hits\"] = \"\"\n",
      "        hit_defs = set()\n",
      "        for aln in record.alignments:\n",
      "            for hsp in aln.hsps:\n",
      "                query_perc = ((hsp.query_end - hsp.query_start)+1)*100.0/record.query_length\n",
      "                query_percs.append(query_perc)\n",
      "                query_lens.append(record.query_length)\n",
      "                id_perc = hsp.identities*100.0/hsp.align_length\n",
      "                id_percs.append(id_perc)\n",
      "                if query_perc >= query_min and id_perc >= id_min:\n",
      "                    hit_defs.add(\"%s:%d:%d\" % (aln.hit_def, hsp.sbjct_start, hsp.sbjct_end))\n",
      "                    good_hits += 1\n",
      "                    good_hit_info['len'].append(record.query_length)\n",
      "                    good_hit_info['len_perc'].append(query_perc)\n",
      "                    good_hit_info['id_perc'].append(id_perc)\n",
      "        if len(hit_defs) > 0:\n",
      "            good_queries += 1\n",
      "            snp_data.loc[record.query,\"hits\"] = \"|\".join(hit_defs)\n",
      "    total_recs += 1\n",
      "#     if i == 100:\n",
      "#         break\n",
      "\n",
      "#make sure blank columns are nan\n",
      "snp_data.hits = snp_data.hits.replace(\"\", np.nan)\n",
      "\n",
      "timer.stop()\n",
      "print \"found %d good hits (%d queries) out of %d records (%.2f%%) in %s \" % (good_hits, \n",
      "                                                                             good_queries,\n",
      "                                                                             total_recs, \n",
      "                                                                             good_queries*100.0/total_recs, \n",
      "                                                                             timer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "found 5853 good hits out of 20655 records (28.34%) in 62.5211009979 sec "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = plt.gcf()\n",
      "f.set_size_inches(15, 5)\n",
      "plt.subplot(1,3,1)\n",
      "plt.hist(query_lens)\n",
      "plt.title(\"query lens (%.2f, [%d,%d])\" % (np.mean(query_lens), np.min(query_lens), np.max(query_lens)))\n",
      "plt.subplot(1,3,2)\n",
      "plt.hist(query_percs)\n",
      "plt.title(\"query percent (%.2f, [%d,%d])\" % (np.mean(query_percs), np.min(query_percs), np.max(query_percs)))\n",
      "plt.xlabel(\"n = %d\" % len(query_percs))\n",
      "plt.subplot(1,3,3)\n",
      "plt.hist(id_percs)\n",
      "plt.title(\"identity percent (%.2f, [%d,%d])\" % (np.mean(id_percs), np.min(id_percs), np.max(id_percs)))\n",
      "plt.xlabel(\"n = %d\" % len(id_percs))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query_lens = good_hit_info['len']\n",
      "query_percs = good_hit_info['len_perc']\n",
      "id_percs = good_hit_info['id_perc']\n",
      "f = plt.gcf()\n",
      "f.set_size_inches(15, 5)\n",
      "plt.subplot(1,3,1)\n",
      "plt.hist(query_lens)\n",
      "plt.title(\"query lens (%.2f +/- %.2f [%d,%d])\" % (np.mean(query_lens), np.std(query_lens), np.min(query_lens), np.max(query_lens)))\n",
      "plt.subplot(1,3,2)\n",
      "plt.hist(query_percs)\n",
      "plt.title(\"query percent (%.2f +/- %.2f [%d,%d])\" % (np.mean(query_percs), np.std(query_percs), np.min(query_percs), np.max(query_percs)))\n",
      "plt.xlabel(\"n = %d\" % len(query_percs))\n",
      "plt.subplot(1,3,3)\n",
      "plt.hist(id_percs)\n",
      "plt.title(\"identity percent (%.2f +/- %.2f [%d,%d])\" % (np.mean(id_percs), np.std(id_percs), np.min(id_percs), np.max(id_percs)))\n",
      "plt.xlabel(\"n = %d\" % len(id_percs))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "snp_data.hits"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hit_set = set()\n",
      "for x in snp_data.hits.dropna():\n",
      "    x = x.split(\"|\")\n",
      "    [hit_set.add(y) for y in x]\n",
      "print len(hit_set), len(snp_data.hits.dropna())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Example output of contigs with SNPs that have blast hits to loblolly genome\n",
      "\n",
      "    rad_tag_id\n",
      "    NODE_106563_length_63_cov_3.555556     tscaffold4974:694196:694271\n",
      "    NODE_108357_length_63_cov_2.047619     tscaffold5167:183586:183676\n",
      "    NODE_108358_length_63_cov_2.936508                             NaN\n",
      "    NODE_108968_length_63_cov_2.000000                             NaN\n",
      "    NODE_111780_length_63_cov_2.000000           C32232780:10467:10543\n",
      "    NODE_112408_length_63_cov_1.206349                             NaN\n",
      "    NODE_118998_length_63_cov_2.000000                             NaN\n",
      "    NODE_122873_length_83_cov_3.168675                             NaN\n",
      "    NODE_124228_length_63_cov_2.000000                             NaN\n",
      "    NODE_125000_length_75_cov_1.866667     tscaffold5160:400260:400358\n",
      "    NODE_126203_length_63_cov_3.000000                             NaN\n",
      "    NODE_126903_length_63_cov_2.825397      scaffold894934:18837:18923\n",
      "    NODE_12700_length_63_cov_2.698413                              NaN\n",
      "    NODE_130912_length_63_cov_2.000000                             NaN\n",
      "    NODE_140818_length_63_cov_2.000000    scaffold327908.3:79145:79225\n",
      "    ...\n",
      "    NODE_732387_length_63_cov_2.634921                             NaN\n",
      "    NODE_745122_length_63_cov_2.079365                             NaN\n",
      "    NODE_74917_length_63_cov_2.000000                              NaN\n",
      "    NODE_7528_length_63_cov_2.000000                               NaN\n",
      "    NODE_76041_length_63_cov_2.619048                              NaN\n",
      "    NODE_761497_length_63_cov_2.460317    scaffold765983:184541:184602\n",
      "    NODE_764083_length_63_cov_4.000000               C29990318:356:268\n",
      "    NODE_774944_length_62_cov_1.000000    scaffold606848:138537:138627\n",
      "    NODE_776867_length_63_cov_2.000000                             NaN\n",
      "    NODE_80918_length_63_cov_3.492064                              NaN\n",
      "    NODE_89989_length_63_cov_2.000000                              NaN\n",
      "    NODE_94091_length_63_cov_2.000000                              NaN\n",
      "    NODE_94099_length_63_cov_3.000000                              NaN\n",
      "    NODE_95100_length_63_cov_4.000000                              NaN\n",
      "    NODE_97204_length_89_cov_1.865169                              NaN\n",
      "    Name: hits, Length: 20655, dtype: object"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gff_dir = \"/home/cfriedline\"\n",
      "gff_files = ['ptaeda.v1.01.scaffolds.trimmed.all.genes.highq_partial.gff',\n",
      "             'ptaeda.v1.01.scaffolds.trimmed.all.genes.highq_whole.gff',\n",
      "             'ptaeda.v1.01.scaffolds.trimmed.all.genes.lowq_whole.gff',\n",
      "             'ptaeda.v1.01.scaffolds.trimmed.all.genes.lowq_partial.gff']\n",
      "gff_files = [os.path.join(gff_dir, x) for x in gff_files]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#this cell crushes the kernel\n",
      "timer = stopwatch.Timer()\n",
      "gff_data = {}\n",
      "for gff in gff_files:\n",
      "    print gff\n",
      "    inner = {}\n",
      "    inner['data'] = list(GFF.parse(gff))\n",
      "    gff_data[os.path.basename(gff)] = inner\n",
      "timer.stop()\n",
      "print timer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for key, v in gff_data.items():\n",
      "    data = v['data']\n",
      "    index = {}\n",
      "    v['index'] = index\n",
      "    print key, len(data)\n",
      "    for elem in data:\n",
      "        index[elem.id] = elem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def split_hit(series):\n",
      "    try:\n",
      "        float(series.hits)\n",
      "        return series.hits\n",
      "    except:\n",
      "        return [x.split(\":\") for x in str(series.hits).split(\"|\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hits = snp_data.apply(split_hit, axis=1).dropna()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hits[0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    rad_tag_id\n",
      "    NODE_106563_length_63_cov_3.555556    [[tscaffold4974, 694196, 694271]]\n",
      "    NODE_108357_length_63_cov_2.047619    [[tscaffold5167, 183586, 183676]]\n",
      "    NODE_111780_length_63_cov_2.000000          [[C32232780, 10467, 10543]]\n",
      "    NODE_125000_length_75_cov_1.866667    [[tscaffold5160, 400260, 400358]]\n",
      "    NODE_126903_length_63_cov_2.825397     [[scaffold894934, 18837, 18923]]\n",
      "    dtype: object"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 0\n",
      "scaffolds = set()\n",
      "for hit in hits:\n",
      "    index = hits.index[count]\n",
      "    hit_anno = {}\n",
      "    hit_feat = {}\n",
      "    hit_type = {}\n",
      "    for triplet in hit:\n",
      "        name, start, end = triplet\n",
      "        hit_coords = sorted([int(start), int(end)])\n",
      "        for key, data in gff_data.items():\n",
      "            if name in data['index']:\n",
      "                rec = data['index'][name]\n",
      "                for f in rec.features:\n",
      "                    genome_coords = sorted([f.location.nofuzzy_start, f.location.nofuzzy_end])\n",
      "                    if (hit_coords[0] >= genome_coords[0]) and (hit_coords[1] <= genome_coords[1]):\n",
      "                        if not key in hit_anno:\n",
      "                            hit_anno[key] = []\n",
      "                        hit_anno[key].append(f.qualifiers)\n",
      "                        scaffolds.add(name)\n",
      "                        \n",
      "                        if not key in hit_feat:\n",
      "                            hit_feat[key] = []\n",
      "                        hit_feat[key].append(f)\n",
      "                        \n",
      "                    if not key in hit_type:\n",
      "                        hit_type[key] = set()\n",
      "                    hit_type[key].add(f.type)\n",
      "                    \n",
      "    snp_data.ix[index, 'annotation'] = hit_anno.__repr__()    \n",
      "    snp_data.ix[index, 'type'] = hit_type.values().__repr__()\n",
      "    snp_data.ix[index, 'features'] = hit_feat.__repr__()\n",
      "    count+=1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ontology_terms = set()\n",
      "for x in snp_data.annotation:\n",
      "    if pd.notnull(x):\n",
      "        x = eval(x)\n",
      "        for key, v in x.items():\n",
      "            if 'Ontology_term' in v[0]:\n",
      "                for ot in v[0]['Ontology_term']:\n",
      "                    ontology_terms.add(ot)\n",
      "print \"number of unique ontology terms=%d, number of unique scaffolds=%d\" % (len(ontology_terms),\n",
      "                                                                           len(scaffolds)) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    number of unique ontology terms=303, number of unique scaffolds=587"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_row(series):\n",
      "    if pd.notnull(series.dbxref):\n",
      "        series.dbxref = '|'.join(eval(series.dbxref))\n",
      "    else:\n",
      "        series.dbxref = \"\"\n",
      "        \n",
      "    if pd.notnull(series.ontology_term):\n",
      "        series.ontology_term = '|'.join(eval(series.ontology_term))\n",
      "    else:\n",
      "        series.dbxref = \"\"\n",
      "    return series"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_data():\n",
      "    f = os.path.basename(snp_file+\"_mod.txt\")\n",
      "    c = snp_data.drop('annotation', axis=1)\n",
      "    c = snp_data.drop('type', axis=1)\n",
      "    c = snp_data.drop(\"features\", axis=1)\n",
      "    #c = snp_data.apply(clean_row, axis=1)\n",
      "    c.to_csv(f, sep=\"\\t\")\n",
      "    return FileLink(f)\n",
      "write_data()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scaffolds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}